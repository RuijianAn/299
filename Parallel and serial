import pycuda.driver as cuda
import pycuda.autoinit
from pycuda.compiler import SourceModule
import time
import numpy

"""
The graph G = (V, E) is represented by a matrix A.
The vertex in the graph is numbered from 0,1,..., |V|-1, and
A[i][j] represents the weight of the edge between vertex i and vertex j.

In the Bellman-Ford algorithm, each vertex has an attribute 'd' to record
the distance from itself to the source vertex at the moment.
I use an array 'dist' to record it where dist[i] represents the attribute 'd' of vertex i."
"""


#Initilization:
#Set the attribute 'd' of all the vertices except source vertex to be INFINITY.
#Set the attribute 'd' of source index to be 0.
initi = SourceModule("""
  __global__ void ini(float *a, int i)
  {
    #include <math.h>
    int idx = threadIdx.x;
    a[idx] = INFINITY;
    if (idx == i){
       a[idx] = 0.0;
    }      
  }
  """)

initilization = initi.get_function("ini")



#Relax:
#For all vertices in the graph, update the attribute 'd'.
#For vertex j, replace the value of dist[j] with min(dist[j], dist[i] + A[i][j])
#where i is the vertex that is adjacent to j and minimize the dist[i] + A[i][j].
relax = SourceModule("""
      __global__ void relaxa(float *dist, float *matrix, int v)
      {
                int idx = threadIdx.x;
		float minimum = dist[idx];
		int i;
		for (i=0;i<v;i++){
		    if (i != idx){
			if (dist[i] + matrix[i + v * idx] < minimum){
			    minimum = dist[i] + matrix[i + v * idx]; 
			}
		    }
		}
		dist[idx] = minimum;
	}
	""")

relaxation = relax.get_function("relaxa")


#The Bellman-Ford algorithm in serial.
def bellman_ford(dist, matrix, idx):
    """
    Run the bellman-ford algorithm in serial.

    @param dist: list
    @param matrix: list[list]
    @param idx: int
    @rtype: list
    """
    l = len(dist)

    #Initialization
    for i in range(l):
        dist[i] = float('inf')
        if i == idx:
            dist[i] = 0

    #Relaxation
    for j in range(l - 1):
        for k in range(l):
            for m in range(l):
                if m != k and dist[m] + matrix[k][m] < dist[k]:
                    dist[k] = dist[m] + matrix[k][m]

    return dist


#The number of vertices in the graph.
num_of_vertex = 500

#The number of vertices in the graph which can be passed into kernel.
v_numpy = numpy.int32(num_of_vertex)

#Create a dist array.
dist = numpy.random.rand(num_of_vertex)
dist = dist.astype(numpy.float32)

#Transfer the dist to GPU.
dist_gpu = cuda.mem_alloc(dist.nbytes)
cuda.memcpy_htod(dist_gpu, dist)

#Create a random matrix.
matrix = numpy.random.rand(num_of_vertex, num_of_vertex)
matrix = matrix.astype(numpy.float32)

#Transfer the matrix to GPU.
matrix_gpu = cuda.mem_alloc(matrix.nbytes)
cuda.memcpy_htod(matrix_gpu, matrix)

#Declare the index of the source vertex.
src_idx = numpy.int32(0)

#Record the start time for GPU.
start_time_gpu = time.time()
#Initialize the dist_gpu.
initilization(dist_gpu, src_idx, block=(num_of_vertex,1,1))

#Relax each edge for |V| - 1 times.
for i in range(num_of_vertex - 1):
    relaxation(dist_gpu, matrix_gpu, v_numpy, block=(num_of_vertex,1,1))

#Copy the dist back to CPU.
dist_result = numpy.empty_like(dist)
cuda.memcpy_dtoh(dist_result, dist_gpu)

#Print the result and running time.
print matrix
print dist_result
print("--- %s seconds ---" % (time.time() - start_time_gpu))



#Record the start time for CPU.
start_time_cpu = time.time()

#Print the result and running time.
print bellman_ford(dist, matrix, 0)
print("--- %s seconds ---" % (time.time() - start_time_cpu))
